This project's main goal was to create a reliable Python script that could handle large CSV files quickly and effectively while carrying out common SQL query operations.
Developing scalable, efficient, and modular code to carry out various data manipulation tasks was the aim.

1) Chunk-Based Processing: Recognizing the importance of chunk-based processing and how it helps with effective memory management when working with large CSV files. Memory Size is assumed to be 3000 rows and Chunks are of 1000 rows
2) Custom Query Execution: By including functions that can parse and run SQL-like queries on CSV data, users will be able to manipulate data in a variety of ways.
3) Modular code design: Creating code modules with readability, maintainability, and scalability in mind so that functions can be reused for a variety of data manipulation tasks.
4) Crime Dataset: 7 columns and 10,000 rows
5) LA_Crime_Dataset 2022: The project was tested on this data as well. 9 Columns and 10,000 rows Memory Size is assumed to be 3000 rows and Chunks are of 1000 rows
